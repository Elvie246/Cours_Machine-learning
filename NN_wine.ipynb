{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.18.0-cp311-cp311-win_amd64.whl.metadata (3.3 kB)\n",
      "Collecting tensorflow-intel==2.18.0 (from tensorflow)\n",
      "  Downloading tensorflow_intel-2.18.0-cp311-cp311-win_amd64.whl.metadata (4.9 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.2)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading protobuf-5.28.3-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Collecting requests<3,>=2.21.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: setuptools in c:\\program files\\windowsapps\\pythonsoftwarefoundation.python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.12.2)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading wrapt-1.16.0-cp311-cp311-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading grpcio-1.68.0-cp311-cp311-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading keras-3.6.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting numpy<2.1.0,>=1.26.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading numpy-2.0.2-cp311-cp311-win_amd64.whl.metadata (59 kB)\n",
      "Collecting h5py>=3.11.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading h5py-3.12.1-cp311-cp311-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting ml-dtypes<0.5.0,>=0.4.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading ml_dtypes-0.4.1-cp311-cp311-win_amd64.whl.metadata (20 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl.metadata (14 kB)\n",
      "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading wheel-0.45.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting rich (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading optree-0.13.1-cp311-cp311-win_amd64.whl.metadata (48 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading charset_normalizer-3.4.0-cp311-cp311-win_amd64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading MarkupSafe-3.0.2-cp311-cp311-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading tensorflow-2.18.0-cp311-cp311-win_amd64.whl (7.5 kB)\n",
      "Downloading tensorflow_intel-2.18.0-cp311-cp311-win_amd64.whl (390.2 MB)\n",
      "   ---------------------------------------- 0.0/390.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.5/390.2 MB 3.4 MB/s eta 0:01:56\n",
      "   ---------------------------------------- 1.3/390.2 MB 3.9 MB/s eta 0:01:39\n",
      "   ---------------------------------------- 2.4/390.2 MB 3.8 MB/s eta 0:01:42\n",
      "   ---------------------------------------- 3.7/390.2 MB 4.4 MB/s eta 0:01:27\n",
      "   ---------------------------------------- 4.7/390.2 MB 4.8 MB/s eta 0:01:20\n",
      "    --------------------------------------- 5.8/390.2 MB 4.5 MB/s eta 0:01:26\n",
      "    --------------------------------------- 6.6/390.2 MB 4.4 MB/s eta 0:01:27\n",
      "    --------------------------------------- 8.7/390.2 MB 5.0 MB/s eta 0:01:17\n",
      "    --------------------------------------- 9.4/390.2 MB 5.0 MB/s eta 0:01:17\n",
      "   - -------------------------------------- 10.7/390.2 MB 5.0 MB/s eta 0:01:16\n",
      "   - -------------------------------------- 11.8/390.2 MB 5.0 MB/s eta 0:01:16\n",
      "   - -------------------------------------- 12.8/390.2 MB 5.0 MB/s eta 0:01:16\n",
      "   - -------------------------------------- 14.2/390.2 MB 5.1 MB/s eta 0:01:15\n",
      "   - -------------------------------------- 15.7/390.2 MB 5.2 MB/s eta 0:01:12\n",
      "   - -------------------------------------- 16.8/390.2 MB 5.2 MB/s eta 0:01:12\n",
      "   - -------------------------------------- 18.1/390.2 MB 5.3 MB/s eta 0:01:11\n",
      "   - -------------------------------------- 18.6/390.2 MB 5.2 MB/s eta 0:01:12\n",
      "   -- ------------------------------------- 19.7/390.2 MB 5.1 MB/s eta 0:01:13\n",
      "   -- ------------------------------------- 20.7/390.2 MB 5.2 MB/s eta 0:01:12\n",
      "   -- ------------------------------------- 22.0/390.2 MB 5.2 MB/s eta 0:01:12\n",
      "   -- ------------------------------------- 23.6/390.2 MB 5.2 MB/s eta 0:01:10\n",
      "   -- ------------------------------------- 25.2/390.2 MB 5.3 MB/s eta 0:01:09\n",
      "   -- ------------------------------------- 26.7/390.2 MB 5.4 MB/s eta 0:01:07\n",
      "   -- ------------------------------------- 28.0/390.2 MB 5.4 MB/s eta 0:01:07\n",
      "   -- ------------------------------------- 29.1/390.2 MB 5.5 MB/s eta 0:01:07\n",
      "   --- ------------------------------------ 30.7/390.2 MB 5.5 MB/s eta 0:01:06\n",
      "   --- ------------------------------------ 31.7/390.2 MB 5.5 MB/s eta 0:01:06\n",
      "   --- ------------------------------------ 32.8/390.2 MB 5.4 MB/s eta 0:01:06\n",
      "   --- ------------------------------------ 33.8/390.2 MB 5.4 MB/s eta 0:01:06\n",
      "   --- ------------------------------------ 34.6/390.2 MB 5.4 MB/s eta 0:01:06\n",
      "   --- ------------------------------------ 36.2/390.2 MB 5.4 MB/s eta 0:01:06\n",
      "   --- ------------------------------------ 37.5/390.2 MB 5.5 MB/s eta 0:01:05\n",
      "   ---- ----------------------------------- 39.1/390.2 MB 5.5 MB/s eta 0:01:04\n",
      "   ---- ----------------------------------- 39.8/390.2 MB 5.5 MB/s eta 0:01:04\n",
      "   ---- ----------------------------------- 41.4/390.2 MB 5.5 MB/s eta 0:01:04\n",
      "   ---- ----------------------------------- 41.9/390.2 MB 5.5 MB/s eta 0:01:04\n",
      "   ---- ----------------------------------- 43.5/390.2 MB 5.5 MB/s eta 0:01:04\n",
      "   ---- ----------------------------------- 45.1/390.2 MB 5.5 MB/s eta 0:01:03\n",
      "   ---- ----------------------------------- 45.6/390.2 MB 5.5 MB/s eta 0:01:04\n",
      "   ---- ----------------------------------- 47.7/390.2 MB 5.6 MB/s eta 0:01:02\n",
      "   ---- ----------------------------------- 48.5/390.2 MB 5.6 MB/s eta 0:01:02\n",
      "   ----- ---------------------------------- 49.5/390.2 MB 5.5 MB/s eta 0:01:02\n",
      "   ----- ---------------------------------- 51.4/390.2 MB 5.6 MB/s eta 0:01:01\n",
      "   ----- ---------------------------------- 51.6/390.2 MB 5.5 MB/s eta 0:01:02\n",
      "   ----- ---------------------------------- 53.0/390.2 MB 5.5 MB/s eta 0:01:01\n",
      "   ----- ---------------------------------- 54.8/390.2 MB 5.6 MB/s eta 0:01:01\n",
      "   ----- ---------------------------------- 56.1/390.2 MB 5.6 MB/s eta 0:01:00\n",
      "   ----- ---------------------------------- 57.4/390.2 MB 5.6 MB/s eta 0:01:00\n",
      "   ------ --------------------------------- 59.0/390.2 MB 5.6 MB/s eta 0:00:59\n",
      "   ------ --------------------------------- 60.6/390.2 MB 5.6 MB/s eta 0:00:59\n",
      "   ------ --------------------------------- 61.9/390.2 MB 5.7 MB/s eta 0:00:59\n",
      "   ------ --------------------------------- 63.4/390.2 MB 5.7 MB/s eta 0:00:58\n",
      "   ------ --------------------------------- 65.0/390.2 MB 5.7 MB/s eta 0:00:57\n",
      "   ------ --------------------------------- 66.6/390.2 MB 5.7 MB/s eta 0:00:57\n",
      "   ------ --------------------------------- 67.9/390.2 MB 5.7 MB/s eta 0:00:57\n",
      "   ------- -------------------------------- 68.7/390.2 MB 5.7 MB/s eta 0:00:57\n",
      "   ------- -------------------------------- 69.5/390.2 MB 5.7 MB/s eta 0:00:57\n",
      "   ------- -------------------------------- 70.5/390.2 MB 5.7 MB/s eta 0:00:57\n",
      "   ------- -------------------------------- 71.6/390.2 MB 5.7 MB/s eta 0:00:57\n",
      "   ------- -------------------------------- 72.1/390.2 MB 5.6 MB/s eta 0:00:57\n",
      "   ------- -------------------------------- 73.4/390.2 MB 5.6 MB/s eta 0:00:57\n",
      "   ------- -------------------------------- 75.2/390.2 MB 5.6 MB/s eta 0:00:56\n",
      "   ------- -------------------------------- 76.5/390.2 MB 5.6 MB/s eta 0:00:56\n",
      "   -------- ------------------------------- 78.1/390.2 MB 5.7 MB/s eta 0:00:56\n",
      "   -------- ------------------------------- 78.9/390.2 MB 5.6 MB/s eta 0:00:56\n",
      "   -------- ------------------------------- 79.4/390.2 MB 5.6 MB/s eta 0:00:56\n",
      "   -------- ------------------------------- 81.3/390.2 MB 5.6 MB/s eta 0:00:55\n",
      "   -------- ------------------------------- 82.3/390.2 MB 5.6 MB/s eta 0:00:55\n",
      "   -------- ------------------------------- 84.1/390.2 MB 5.7 MB/s eta 0:00:55\n",
      "   -------- ------------------------------- 85.2/390.2 MB 5.6 MB/s eta 0:00:55\n",
      "   -------- ------------------------------- 86.5/390.2 MB 5.7 MB/s eta 0:00:54\n",
      "   -------- ------------------------------- 87.6/390.2 MB 5.6 MB/s eta 0:00:54\n",
      "   --------- ------------------------------ 88.9/390.2 MB 5.7 MB/s eta 0:00:54\n",
      "   --------- ------------------------------ 90.4/390.2 MB 5.7 MB/s eta 0:00:53\n",
      "   --------- ------------------------------ 91.8/390.2 MB 5.7 MB/s eta 0:00:53\n",
      "   --------- ------------------------------ 92.8/390.2 MB 5.7 MB/s eta 0:00:53\n",
      "   --------- ------------------------------ 94.4/390.2 MB 5.7 MB/s eta 0:00:52\n",
      "   --------- ------------------------------ 95.9/390.2 MB 5.7 MB/s eta 0:00:52\n",
      "   --------- ------------------------------ 97.0/390.2 MB 5.7 MB/s eta 0:00:52\n",
      "   ---------- ----------------------------- 98.3/390.2 MB 5.7 MB/s eta 0:00:52\n",
      "   ---------- ----------------------------- 99.6/390.2 MB 5.7 MB/s eta 0:00:51\n",
      "   ---------- ----------------------------- 100.9/390.2 MB 5.7 MB/s eta 0:00:51\n",
      "   ---------- ----------------------------- 101.7/390.2 MB 5.7 MB/s eta 0:00:51\n",
      "   ---------- ----------------------------- 103.0/390.2 MB 5.7 MB/s eta 0:00:51\n",
      "   ---------- ----------------------------- 104.3/390.2 MB 5.7 MB/s eta 0:00:51\n",
      "   ---------- ----------------------------- 106.2/390.2 MB 5.7 MB/s eta 0:00:50\n",
      "   ----------- ---------------------------- 107.5/390.2 MB 5.8 MB/s eta 0:00:50\n",
      "   ----------- ---------------------------- 108.8/390.2 MB 5.7 MB/s eta 0:00:49\n",
      "   ----------- ---------------------------- 109.1/390.2 MB 5.7 MB/s eta 0:00:50\n",
      "   ----------- ---------------------------- 109.8/390.2 MB 5.7 MB/s eta 0:00:50\n",
      "   ----------- ---------------------------- 111.1/390.2 MB 5.7 MB/s eta 0:00:50\n",
      "   ----------- ---------------------------- 112.2/390.2 MB 5.6 MB/s eta 0:00:50\n",
      "   ----------- ---------------------------- 113.5/390.2 MB 5.7 MB/s eta 0:00:49\n",
      "   ----------- ---------------------------- 114.8/390.2 MB 5.7 MB/s eta 0:00:49\n",
      "   ----------- ---------------------------- 115.6/390.2 MB 5.6 MB/s eta 0:00:49\n",
      "   ------------ --------------------------- 117.2/390.2 MB 5.6 MB/s eta 0:00:49\n",
      "   ------------ --------------------------- 118.2/390.2 MB 5.6 MB/s eta 0:00:49\n",
      "   ------------ --------------------------- 119.0/390.2 MB 5.6 MB/s eta 0:00:49\n",
      "   ------------ --------------------------- 120.6/390.2 MB 5.6 MB/s eta 0:00:48\n",
      "   ------------ --------------------------- 121.6/390.2 MB 5.6 MB/s eta 0:00:48\n",
      "   ------------ --------------------------- 122.9/390.2 MB 5.6 MB/s eta 0:00:48\n",
      "   ------------ --------------------------- 124.0/390.2 MB 5.6 MB/s eta 0:00:48\n",
      "   ------------ --------------------------- 125.3/390.2 MB 5.6 MB/s eta 0:00:47\n",
      "   ------------- -------------------------- 126.9/390.2 MB 5.7 MB/s eta 0:00:47\n",
      "   ------------- -------------------------- 127.9/390.2 MB 5.7 MB/s eta 0:00:47\n",
      "   ------------- -------------------------- 129.0/390.2 MB 5.6 MB/s eta 0:00:47\n",
      "   ------------- -------------------------- 130.0/390.2 MB 5.6 MB/s eta 0:00:47\n",
      "   ------------- -------------------------- 131.9/390.2 MB 5.7 MB/s eta 0:00:46\n",
      "   ------------- -------------------------- 132.9/390.2 MB 5.7 MB/s eta 0:00:46\n",
      "   ------------- -------------------------- 134.2/390.2 MB 5.7 MB/s eta 0:00:46\n",
      "   ------------- -------------------------- 135.0/390.2 MB 5.6 MB/s eta 0:00:46\n",
      "   -------------- ------------------------- 136.6/390.2 MB 5.7 MB/s eta 0:00:45\n",
      "   -------------- ------------------------- 138.1/390.2 MB 5.7 MB/s eta 0:00:45\n",
      "   -------------- ------------------------- 138.7/390.2 MB 5.7 MB/s eta 0:00:45\n",
      "   -------------- ------------------------- 139.7/390.2 MB 5.6 MB/s eta 0:00:45\n",
      "   -------------- ------------------------- 141.0/390.2 MB 5.6 MB/s eta 0:00:45\n",
      "   -------------- ------------------------- 141.8/390.2 MB 5.6 MB/s eta 0:00:45\n",
      "   -------------- ------------------------- 142.3/390.2 MB 5.6 MB/s eta 0:00:45\n",
      "   -------------- ------------------------- 143.1/390.2 MB 5.6 MB/s eta 0:00:45\n",
      "   -------------- ------------------------- 144.4/390.2 MB 5.6 MB/s eta 0:00:45\n",
      "   -------------- ------------------------- 145.0/390.2 MB 5.6 MB/s eta 0:00:45\n",
      "   -------------- ------------------------- 145.8/390.2 MB 5.5 MB/s eta 0:00:45\n",
      "   -------------- ------------------------- 146.3/390.2 MB 5.5 MB/s eta 0:00:45\n",
      "   --------------- ------------------------ 147.1/390.2 MB 5.5 MB/s eta 0:00:45\n",
      "   --------------- ------------------------ 148.1/390.2 MB 5.5 MB/s eta 0:00:45\n",
      "   --------------- ------------------------ 149.2/390.2 MB 5.5 MB/s eta 0:00:44\n",
      "   --------------- ------------------------ 150.2/390.2 MB 5.5 MB/s eta 0:00:44\n",
      "   --------------- ------------------------ 151.0/390.2 MB 5.5 MB/s eta 0:00:44\n",
      "   --------------- ------------------------ 152.3/390.2 MB 5.5 MB/s eta 0:00:44\n",
      "   --------------- ------------------------ 153.1/390.2 MB 5.5 MB/s eta 0:00:44\n",
      "   --------------- ------------------------ 154.1/390.2 MB 5.5 MB/s eta 0:00:44\n",
      "   --------------- ------------------------ 155.2/390.2 MB 5.4 MB/s eta 0:00:44\n",
      "   ---------------- ----------------------- 156.5/390.2 MB 5.4 MB/s eta 0:00:43\n",
      "   ---------------- ----------------------- 158.3/390.2 MB 5.5 MB/s eta 0:00:43\n",
      "   ---------------- ----------------------- 159.4/390.2 MB 5.5 MB/s eta 0:00:43\n",
      "   ---------------- ----------------------- 159.9/390.2 MB 5.5 MB/s eta 0:00:43\n",
      "   ---------------- ----------------------- 161.2/390.2 MB 5.5 MB/s eta 0:00:42\n",
      "   ---------------- ----------------------- 162.5/390.2 MB 5.5 MB/s eta 0:00:42\n",
      "   ---------------- ----------------------- 163.6/390.2 MB 5.4 MB/s eta 0:00:42\n",
      "   ---------------- ----------------------- 164.9/390.2 MB 5.5 MB/s eta 0:00:42\n",
      "   ----------------- ---------------------- 165.9/390.2 MB 5.5 MB/s eta 0:00:41\n",
      "   ----------------- ---------------------- 167.5/390.2 MB 5.5 MB/s eta 0:00:41\n",
      "   ----------------- ---------------------- 168.8/390.2 MB 5.5 MB/s eta 0:00:41\n",
      "   ----------------- ---------------------- 170.1/390.2 MB 5.5 MB/s eta 0:00:41\n",
      "   ----------------- ---------------------- 173.0/390.2 MB 5.5 MB/s eta 0:00:40\n",
      "   ----------------- ---------------------- 173.8/390.2 MB 5.5 MB/s eta 0:00:40\n",
      "   ----------------- ---------------------- 174.9/390.2 MB 5.5 MB/s eta 0:00:40\n",
      "   ------------------ --------------------- 176.4/390.2 MB 5.5 MB/s eta 0:00:39\n",
      "   ------------------ --------------------- 177.2/390.2 MB 5.5 MB/s eta 0:00:39\n",
      "   ------------------ --------------------- 178.3/390.2 MB 5.5 MB/s eta 0:00:39\n",
      "   ------------------ --------------------- 179.8/390.2 MB 5.5 MB/s eta 0:00:39\n",
      "   ------------------ --------------------- 180.9/390.2 MB 5.5 MB/s eta 0:00:39\n",
      "   ------------------ --------------------- 182.2/390.2 MB 5.5 MB/s eta 0:00:38\n",
      "   ------------------ --------------------- 183.0/390.2 MB 5.5 MB/s eta 0:00:38\n",
      "   ------------------ --------------------- 184.0/390.2 MB 5.5 MB/s eta 0:00:38\n",
      "   ------------------- -------------------- 185.6/390.2 MB 5.5 MB/s eta 0:00:38\n",
      "   ------------------- -------------------- 186.4/390.2 MB 5.5 MB/s eta 0:00:38\n",
      "   ------------------- -------------------- 187.2/390.2 MB 5.5 MB/s eta 0:00:38\n",
      "   ------------------- -------------------- 188.2/390.2 MB 5.4 MB/s eta 0:00:38\n",
      "   ------------------- -------------------- 189.3/390.2 MB 5.5 MB/s eta 0:00:37\n",
      "   ------------------- -------------------- 190.6/390.2 MB 5.4 MB/s eta 0:00:37\n",
      "   ------------------- -------------------- 191.6/390.2 MB 5.4 MB/s eta 0:00:37\n",
      "   ------------------- -------------------- 192.9/390.2 MB 5.4 MB/s eta 0:00:37\n",
      "   ------------------- -------------------- 194.5/390.2 MB 5.4 MB/s eta 0:00:36\n",
      "   -------------------- ------------------- 195.6/390.2 MB 5.4 MB/s eta 0:00:36\n",
      "   -------------------- ------------------- 195.8/390.2 MB 5.4 MB/s eta 0:00:36\n",
      "   -------------------- ------------------- 196.6/390.2 MB 5.4 MB/s eta 0:00:36\n",
      "   -------------------- ------------------- 197.9/390.2 MB 5.4 MB/s eta 0:00:36\n",
      "   -------------------- ------------------- 199.8/390.2 MB 5.4 MB/s eta 0:00:36\n",
      "   -------------------- ------------------- 200.5/390.2 MB 5.4 MB/s eta 0:00:36\n",
      "   -------------------- ------------------- 202.1/390.2 MB 5.4 MB/s eta 0:00:35\n",
      "   -------------------- ------------------- 203.9/390.2 MB 5.4 MB/s eta 0:00:35\n",
      "   -------------------- ------------------- 204.7/390.2 MB 5.4 MB/s eta 0:00:35\n",
      "   --------------------- ------------------ 206.3/390.2 MB 5.5 MB/s eta 0:00:34\n",
      "   --------------------- ------------------ 207.9/390.2 MB 5.5 MB/s eta 0:00:34\n",
      "   --------------------- ------------------ 209.2/390.2 MB 5.5 MB/s eta 0:00:34\n",
      "   --------------------- ------------------ 210.0/390.2 MB 5.4 MB/s eta 0:00:34\n",
      "   --------------------- ------------------ 211.6/390.2 MB 5.4 MB/s eta 0:00:33\n",
      "   --------------------- ------------------ 212.9/390.2 MB 5.4 MB/s eta 0:00:33\n",
      "   --------------------- ------------------ 214.4/390.2 MB 5.4 MB/s eta 0:00:33\n",
      "   ---------------------- ----------------- 215.5/390.2 MB 5.5 MB/s eta 0:00:32\n",
      "   ---------------------- ----------------- 217.3/390.2 MB 5.5 MB/s eta 0:00:32\n",
      "   ---------------------- ----------------- 218.9/390.2 MB 5.5 MB/s eta 0:00:32\n",
      "   ---------------------- ----------------- 220.5/390.2 MB 5.5 MB/s eta 0:00:31\n",
      "   ---------------------- ----------------- 221.5/390.2 MB 5.5 MB/s eta 0:00:31\n",
      "   ---------------------- ----------------- 222.6/390.2 MB 5.4 MB/s eta 0:00:31\n",
      "   ---------------------- ----------------- 223.3/390.2 MB 5.4 MB/s eta 0:00:31\n",
      "   ----------------------- ---------------- 224.4/390.2 MB 5.4 MB/s eta 0:00:31\n",
      "   ----------------------- ---------------- 225.4/390.2 MB 5.4 MB/s eta 0:00:31\n",
      "   ----------------------- ---------------- 227.0/390.2 MB 5.4 MB/s eta 0:00:31\n",
      "   ----------------------- ---------------- 228.3/390.2 MB 5.4 MB/s eta 0:00:30\n",
      "   ----------------------- ---------------- 228.9/390.2 MB 5.4 MB/s eta 0:00:31\n",
      "   ----------------------- ---------------- 230.2/390.2 MB 5.4 MB/s eta 0:00:30\n",
      "   ----------------------- ---------------- 230.7/390.2 MB 5.4 MB/s eta 0:00:30\n",
      "   ----------------------- ---------------- 230.9/390.2 MB 5.4 MB/s eta 0:00:30\n",
      "   ----------------------- ---------------- 232.0/390.2 MB 5.4 MB/s eta 0:00:30\n",
      "   ----------------------- ---------------- 233.0/390.2 MB 5.4 MB/s eta 0:00:30\n",
      "   ----------------------- ---------------- 233.6/390.2 MB 5.4 MB/s eta 0:00:30\n",
      "   ----------------------- ---------------- 233.8/390.2 MB 5.3 MB/s eta 0:00:30\n",
      "   ----------------------- ---------------- 233.8/390.2 MB 5.3 MB/s eta 0:00:30\n",
      "   ------------------------ --------------- 234.6/390.2 MB 5.2 MB/s eta 0:00:30\n",
      "   ------------------------ --------------- 235.7/390.2 MB 5.2 MB/s eta 0:00:30\n",
      "   ------------------------ --------------- 237.5/390.2 MB 5.3 MB/s eta 0:00:29\n",
      "   ------------------------ --------------- 238.0/390.2 MB 5.2 MB/s eta 0:00:30\n",
      "   ------------------------ --------------- 239.3/390.2 MB 5.2 MB/s eta 0:00:29\n",
      "   ------------------------ --------------- 240.6/390.2 MB 5.3 MB/s eta 0:00:29\n",
      "   ------------------------ --------------- 242.0/390.2 MB 5.3 MB/s eta 0:00:29\n",
      "   ------------------------ --------------- 242.0/390.2 MB 5.3 MB/s eta 0:00:29\n",
      "   ------------------------ --------------- 243.8/390.2 MB 5.2 MB/s eta 0:00:29\n",
      "   ------------------------- -------------- 245.4/390.2 MB 5.2 MB/s eta 0:00:28\n",
      "   ------------------------- -------------- 245.9/390.2 MB 5.2 MB/s eta 0:00:28\n",
      "   ------------------------- -------------- 245.9/390.2 MB 5.2 MB/s eta 0:00:28\n",
      "   ------------------------- -------------- 245.9/390.2 MB 5.2 MB/s eta 0:00:28\n",
      "   ------------------------- -------------- 249.3/390.2 MB 5.2 MB/s eta 0:00:28\n",
      "   ------------------------- -------------- 250.3/390.2 MB 5.2 MB/s eta 0:00:28\n",
      "   ------------------------- -------------- 252.2/390.2 MB 5.2 MB/s eta 0:00:27\n",
      "   ------------------------- -------------- 253.0/390.2 MB 5.2 MB/s eta 0:00:27\n",
      "   -------------------------- ------------- 253.8/390.2 MB 5.1 MB/s eta 0:00:27\n",
      "   -------------------------- ------------- 254.5/390.2 MB 5.1 MB/s eta 0:00:27\n",
      "   -------------------------- ------------- 255.1/390.2 MB 5.1 MB/s eta 0:00:27\n",
      "   -------------------------- ------------- 256.1/390.2 MB 5.1 MB/s eta 0:00:27\n",
      "   -------------------------- ------------- 257.2/390.2 MB 5.1 MB/s eta 0:00:27\n",
      "   -------------------------- ------------- 257.9/390.2 MB 5.1 MB/s eta 0:00:27\n",
      "   -------------------------- ------------- 259.3/390.2 MB 5.1 MB/s eta 0:00:26\n",
      "   -------------------------- ------------- 260.3/390.2 MB 5.1 MB/s eta 0:00:26\n",
      "   -------------------------- ------------- 261.4/390.2 MB 5.1 MB/s eta 0:00:26\n",
      "   -------------------------- ------------- 262.4/390.2 MB 5.1 MB/s eta 0:00:26\n",
      "   -------------------------- ------------- 263.2/390.2 MB 5.1 MB/s eta 0:00:25\n",
      "   --------------------------- ------------ 264.5/390.2 MB 5.1 MB/s eta 0:00:25\n",
      "   --------------------------- ------------ 265.8/390.2 MB 5.1 MB/s eta 0:00:25\n",
      "   --------------------------- ------------ 266.9/390.2 MB 5.1 MB/s eta 0:00:25\n",
      "   --------------------------- ------------ 267.6/390.2 MB 5.1 MB/s eta 0:00:25\n",
      "   --------------------------- ------------ 268.4/390.2 MB 5.0 MB/s eta 0:00:25\n",
      "   --------------------------- ------------ 269.5/390.2 MB 5.0 MB/s eta 0:00:24\n",
      "   --------------------------- ------------ 270.3/390.2 MB 5.0 MB/s eta 0:00:24\n",
      "   --------------------------- ------------ 271.8/390.2 MB 5.0 MB/s eta 0:00:24\n",
      "   --------------------------- ------------ 271.8/390.2 MB 5.0 MB/s eta 0:00:24\n",
      "   --------------------------- ------------ 271.8/390.2 MB 5.0 MB/s eta 0:00:24\n",
      "   --------------------------- ------------ 271.8/390.2 MB 5.0 MB/s eta 0:00:24\n",
      "   --------------------------- ------------ 271.8/390.2 MB 5.0 MB/s eta 0:00:24\n",
      "   --------------------------- ------------ 271.8/390.2 MB 5.0 MB/s eta 0:00:24\n",
      "   --------------------------- ------------ 271.8/390.2 MB 5.0 MB/s eta 0:00:24\n",
      "   --------------------------- ------------ 271.8/390.2 MB 5.0 MB/s eta 0:00:24\n",
      "   --------------------------- ------------ 271.8/390.2 MB 5.0 MB/s eta 0:00:24\n",
      "   --------------------------- ------------ 271.8/390.2 MB 5.0 MB/s eta 0:00:24\n",
      "   ---------------------------- ----------- 273.9/390.2 MB 4.7 MB/s eta 0:00:25\n",
      "   ---------------------------- ----------- 281.8/390.2 MB 4.9 MB/s eta 0:00:23\n",
      "   ----------------------------- ---------- 283.4/390.2 MB 4.9 MB/s eta 0:00:22\n",
      "   ----------------------------- ---------- 283.6/390.2 MB 4.9 MB/s eta 0:00:22\n",
      "   ----------------------------- ---------- 284.4/390.2 MB 4.9 MB/s eta 0:00:22\n",
      "   ----------------------------- ---------- 284.4/390.2 MB 4.9 MB/s eta 0:00:22\n",
      "   ----------------------------- ---------- 284.4/390.2 MB 4.9 MB/s eta 0:00:22\n",
      "   ----------------------------- ---------- 284.4/390.2 MB 4.9 MB/s eta 0:00:22\n",
      "   ----------------------------- ---------- 284.4/390.2 MB 4.9 MB/s eta 0:00:22\n",
      "   ----------------------------- ---------- 284.4/390.2 MB 4.9 MB/s eta 0:00:22\n",
      "   ----------------------------- ---------- 285.7/390.2 MB 4.8 MB/s eta 0:00:22\n",
      "   ----------------------------- ---------- 287.6/390.2 MB 4.8 MB/s eta 0:00:22\n",
      "   ----------------------------- ---------- 288.6/390.2 MB 4.8 MB/s eta 0:00:22\n",
      "   ----------------------------- ---------- 290.2/390.2 MB 4.8 MB/s eta 0:00:21\n",
      "   ----------------------------- ---------- 290.2/390.2 MB 4.8 MB/s eta 0:00:21\n",
      "   ----------------------------- ---------- 290.2/390.2 MB 4.8 MB/s eta 0:00:21\n",
      "   ----------------------------- ---------- 291.0/390.2 MB 4.8 MB/s eta 0:00:21\n",
      "   ----------------------------- ---------- 291.2/390.2 MB 4.8 MB/s eta 0:00:21\n",
      "   ------------------------------ --------- 298.3/390.2 MB 4.9 MB/s eta 0:00:19\n",
      "   ------------------------------ --------- 299.1/390.2 MB 4.9 MB/s eta 0:00:19\n",
      "   ------------------------------ --------- 300.4/390.2 MB 4.9 MB/s eta 0:00:19\n",
      "   ------------------------------ --------- 302.0/390.2 MB 4.9 MB/s eta 0:00:18\n",
      "   ------------------------------- -------- 303.3/390.2 MB 5.0 MB/s eta 0:00:18\n",
      "   ------------------------------- -------- 304.1/390.2 MB 5.0 MB/s eta 0:00:18\n",
      "   ------------------------------- -------- 305.4/390.2 MB 4.9 MB/s eta 0:00:18\n",
      "   ------------------------------- -------- 306.2/390.2 MB 4.9 MB/s eta 0:00:18\n",
      "   ------------------------------- -------- 307.5/390.2 MB 4.9 MB/s eta 0:00:17\n",
      "   ------------------------------- -------- 308.8/390.2 MB 5.0 MB/s eta 0:00:17\n",
      "   ------------------------------- -------- 310.1/390.2 MB 4.9 MB/s eta 0:00:17\n",
      "   ------------------------------- -------- 311.7/390.2 MB 4.9 MB/s eta 0:00:16\n",
      "   -------------------------------- ------- 312.7/390.2 MB 5.0 MB/s eta 0:00:16\n",
      "   -------------------------------- ------- 314.3/390.2 MB 5.0 MB/s eta 0:00:16\n",
      "   -------------------------------- ------- 315.4/390.2 MB 5.0 MB/s eta 0:00:16\n",
      "   -------------------------------- ------- 316.4/390.2 MB 4.9 MB/s eta 0:00:15\n",
      "   -------------------------------- ------- 317.2/390.2 MB 4.9 MB/s eta 0:00:15\n",
      "   -------------------------------- ------- 318.5/390.2 MB 4.9 MB/s eta 0:00:15\n",
      "   -------------------------------- ------- 319.0/390.2 MB 5.0 MB/s eta 0:00:15\n",
      "   -------------------------------- ------- 319.8/390.2 MB 4.9 MB/s eta 0:00:15\n",
      "   -------------------------------- ------- 321.4/390.2 MB 4.9 MB/s eta 0:00:15\n",
      "   --------------------------------- ------ 323.0/390.2 MB 4.9 MB/s eta 0:00:14\n",
      "   --------------------------------- ------ 324.0/390.2 MB 4.9 MB/s eta 0:00:14\n",
      "   --------------------------------- ------ 324.0/390.2 MB 4.9 MB/s eta 0:00:14\n",
      "   --------------------------------- ------ 324.0/390.2 MB 4.9 MB/s eta 0:00:14\n",
      "   --------------------------------- ------ 324.0/390.2 MB 4.9 MB/s eta 0:00:14\n",
      "   --------------------------------- ------ 327.9/390.2 MB 4.9 MB/s eta 0:00:13\n",
      "   --------------------------------- ------ 327.9/390.2 MB 4.9 MB/s eta 0:00:13\n",
      "   --------------------------------- ------ 327.9/390.2 MB 4.9 MB/s eta 0:00:13\n",
      "   --------------------------------- ------ 327.9/390.2 MB 4.9 MB/s eta 0:00:13\n",
      "   --------------------------------- ------ 327.9/390.2 MB 4.9 MB/s eta 0:00:13\n",
      "   --------------------------------- ------ 327.9/390.2 MB 4.9 MB/s eta 0:00:13\n",
      "   --------------------------------- ------ 330.6/390.2 MB 4.8 MB/s eta 0:00:13\n",
      "   --------------------------------- ------ 330.6/390.2 MB 4.8 MB/s eta 0:00:13\n",
      "   --------------------------------- ------ 330.6/390.2 MB 4.8 MB/s eta 0:00:13\n",
      "   --------------------------------- ------ 330.8/390.2 MB 4.7 MB/s eta 0:00:13\n",
      "   ---------------------------------- ----- 333.7/390.2 MB 4.7 MB/s eta 0:00:12\n",
      "   ---------------------------------- ----- 335.0/390.2 MB 4.7 MB/s eta 0:00:12\n",
      "   ---------------------------------- ----- 336.1/390.2 MB 4.7 MB/s eta 0:00:12\n",
      "   ---------------------------------- ----- 336.9/390.2 MB 4.7 MB/s eta 0:00:12\n",
      "   ---------------------------------- ----- 337.9/390.2 MB 4.7 MB/s eta 0:00:12\n",
      "   ---------------------------------- ----- 339.0/390.2 MB 4.7 MB/s eta 0:00:11\n",
      "   ---------------------------------- ----- 340.3/390.2 MB 4.7 MB/s eta 0:00:11\n",
      "   ---------------------------------- ----- 341.0/390.2 MB 4.7 MB/s eta 0:00:11\n",
      "   ----------------------------------- ---- 342.1/390.2 MB 4.7 MB/s eta 0:00:11\n",
      "   ----------------------------------- ---- 343.1/390.2 MB 4.7 MB/s eta 0:00:11\n",
      "   ----------------------------------- ---- 344.2/390.2 MB 4.7 MB/s eta 0:00:10\n",
      "   ----------------------------------- ---- 344.5/390.2 MB 4.7 MB/s eta 0:00:10\n",
      "   ----------------------------------- ---- 346.6/390.2 MB 4.6 MB/s eta 0:00:10\n",
      "   ----------------------------------- ---- 347.6/390.2 MB 4.6 MB/s eta 0:00:10\n",
      "   ----------------------------------- ---- 348.4/390.2 MB 4.6 MB/s eta 0:00:10\n",
      "   ----------------------------------- ---- 350.2/390.2 MB 4.6 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 351.5/390.2 MB 4.6 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 352.6/390.2 MB 4.6 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 353.9/390.2 MB 4.6 MB/s eta 0:00:08\n",
      "   ------------------------------------ --- 354.9/390.2 MB 4.6 MB/s eta 0:00:08\n",
      "   ------------------------------------ --- 356.5/390.2 MB 4.6 MB/s eta 0:00:08\n",
      "   ------------------------------------ --- 357.3/390.2 MB 4.6 MB/s eta 0:00:08\n",
      "   ------------------------------------ --- 358.4/390.2 MB 4.6 MB/s eta 0:00:07\n",
      "   ------------------------------------ --- 359.7/390.2 MB 4.6 MB/s eta 0:00:07\n",
      "   ------------------------------------- -- 361.0/390.2 MB 4.6 MB/s eta 0:00:07\n",
      "   ------------------------------------- -- 362.0/390.2 MB 4.6 MB/s eta 0:00:07\n",
      "   ------------------------------------- -- 363.1/390.2 MB 4.6 MB/s eta 0:00:06\n",
      "   ------------------------------------- -- 363.9/390.2 MB 4.6 MB/s eta 0:00:06\n",
      "   ------------------------------------- -- 364.9/390.2 MB 4.6 MB/s eta 0:00:06\n",
      "   ------------------------------------- -- 366.2/390.2 MB 4.6 MB/s eta 0:00:06\n",
      "   ------------------------------------- -- 367.3/390.2 MB 4.6 MB/s eta 0:00:06\n",
      "   ------------------------------------- -- 368.3/390.2 MB 4.6 MB/s eta 0:00:05\n",
      "   ------------------------------------- -- 369.6/390.2 MB 4.6 MB/s eta 0:00:05\n",
      "   -------------------------------------- - 371.2/390.2 MB 4.7 MB/s eta 0:00:05\n",
      "   -------------------------------------- - 372.0/390.2 MB 4.7 MB/s eta 0:00:04\n",
      "   -------------------------------------- - 373.3/390.2 MB 4.7 MB/s eta 0:00:04\n",
      "   -------------------------------------- - 374.9/390.2 MB 4.8 MB/s eta 0:00:04\n",
      "   -------------------------------------- - 375.9/390.2 MB 4.8 MB/s eta 0:00:04\n",
      "   -------------------------------------- - 377.2/390.2 MB 4.8 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 378.3/390.2 MB 4.8 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 379.8/390.2 MB 4.8 MB/s eta 0:00:03\n",
      "   ---------------------------------------  380.6/390.2 MB 4.8 MB/s eta 0:00:03\n",
      "   ---------------------------------------  381.7/390.2 MB 4.8 MB/s eta 0:00:02\n",
      "   ---------------------------------------  382.5/390.2 MB 4.8 MB/s eta 0:00:02\n",
      "   ---------------------------------------  383.0/390.2 MB 4.7 MB/s eta 0:00:02\n",
      "   ---------------------------------------  384.3/390.2 MB 4.8 MB/s eta 0:00:02\n",
      "   ---------------------------------------  384.8/390.2 MB 4.7 MB/s eta 0:00:02\n",
      "   ---------------------------------------  385.9/390.2 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  386.9/390.2 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  387.4/390.2 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  388.5/390.2 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  389.3/390.2 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  390.1/390.2 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  390.1/390.2 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  390.1/390.2 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  390.1/390.2 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  390.1/390.2 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  390.1/390.2 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  390.1/390.2 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  390.1/390.2 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  390.1/390.2 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 390.2/390.2 MB 4.4 MB/s eta 0:00:00\n",
      "Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpcio-1.68.0-cp311-cp311-win_amd64.whl (4.4 MB)\n",
      "   ---------------------------------------- 0.0/4.4 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.8/4.4 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 1.8/4.4 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 3.1/4.4 MB 4.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 4.2/4.4 MB 4.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.4/4.4 MB 4.3 MB/s eta 0:00:00\n",
      "Downloading h5py-3.12.1-cp311-cp311-win_amd64.whl (3.0 MB)\n",
      "   ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 1.0/3.0 MB 4.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.9/3.0 MB 6.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.0/3.0 MB 4.9 MB/s eta 0:00:00\n",
      "Downloading keras-3.6.0-py3-none-any.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   ----------------------------------- ---- 1.0/1.2 MB 10.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.2/1.2 MB 5.4 MB/s eta 0:00:00\n",
      "Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "   ---------------------------------------- 0.0/26.4 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 1.6/26.4 MB 7.0 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 3.4/26.4 MB 8.4 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 5.0/26.4 MB 7.7 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 6.6/26.4 MB 8.0 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 8.4/26.4 MB 8.0 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 10.0/26.4 MB 7.7 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 11.5/26.4 MB 7.7 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 13.1/26.4 MB 7.8 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 14.7/26.4 MB 7.6 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 16.0/26.4 MB 7.6 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 17.3/26.4 MB 7.4 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 19.1/26.4 MB 7.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 21.2/26.4 MB 7.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 23.1/26.4 MB 7.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 24.6/26.4 MB 7.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.0/26.4 MB 7.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 26.4/26.4 MB 7.3 MB/s eta 0:00:00\n",
      "Downloading ml_dtypes-0.4.1-cp311-cp311-win_amd64.whl (126 kB)\n",
      "Downloading numpy-2.0.2-cp311-cp311-win_amd64.whl (15.9 MB)\n",
      "   ---------------------------------------- 0.0/15.9 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 1.3/15.9 MB 8.4 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 3.1/15.9 MB 8.4 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 5.2/15.9 MB 8.8 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 6.8/15.9 MB 8.2 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 8.7/15.9 MB 8.3 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 10.2/15.9 MB 8.1 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 12.6/15.9 MB 9.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 13.6/15.9 MB 8.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 14.9/15.9 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.7/15.9 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.9/15.9 MB 7.0 MB/s eta 0:00:00\n",
      "Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading protobuf-5.28.3-cp310-abi3-win_amd64.whl (431 kB)\n",
      "Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "   ---------------------------------------- 0.0/5.5 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 1.6/5.5 MB 7.0 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 2.4/5.5 MB 7.4 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 2.6/5.5 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.5/5.5 MB 6.6 MB/s eta 0:00:00\n",
      "Downloading tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ----------------------------------- ---- 1.3/1.5 MB 11.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 4.9 MB/s eta 0:00:00\n",
      "Downloading termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading wrapt-1.16.0-cp311-cp311-win_amd64.whl (37 kB)\n",
      "Downloading certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Downloading charset_normalizer-3.4.0-cp311-cp311-win_amd64.whl (101 kB)\n",
      "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Downloading wheel-0.45.0-py3-none-any.whl (72 kB)\n",
      "Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.13.1-cp311-cp311-win_amd64.whl (292 kB)\n",
      "Downloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading MarkupSafe-3.0.2-cp311-cp311-win_amd64.whl (15 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, wheel, urllib3, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, protobuf, optree, opt-einsum, numpy, mdurl, MarkupSafe, markdown, idna, grpcio, google-pasta, gast, charset-normalizer, certifi, absl-py, werkzeug, requests, ml-dtypes, markdown-it-py, h5py, astunparse, tensorboard, rich, keras, tensorflow-intel, tensorflow\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.1.3\n",
      "    Uninstalling numpy-2.1.3:\n",
      "      Successfully uninstalled numpy-2.1.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\LENOVO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\~umpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\LENOVO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\~umpy'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\LENOVO\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python311\\\\site-packages\\\\tensorflow\\\\include\\\\external\\\\com_github_grpc_grpc\\\\src\\\\core\\\\ext\\\\filters\\\\client_channel\\\\lb_policy\\\\grpclb\\\\client_load_reporting_filter.h'\n",
      "HINT: This error might have occurred since this system does not have Windows Long Path support enabled. You can find information on how to enable this at https://pip.pypa.io/warnings/enable-long-paths\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      9.4        5  \n",
       "1      9.8        5  \n",
       "2      9.8        5  \n",
       "3      9.8        6  \n",
       "4      9.4        5  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wine = pd.read_csv('C:/Users/LENOVO/Desktop/Cours_Machine-learning/wine-red.csv')\n",
    "df_wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',\n",
      "       'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',\n",
      "       'pH', 'sulphates', 'alcohol', 'quality'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Check the column names\n",
    "print(df_wine.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class Counts:\n",
      "quality\n",
      "5    681\n",
      "6    638\n",
      "7    199\n",
      "4     53\n",
      "8     18\n",
      "3     10\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count the number of occurrences of each class\n",
    "class_counts = df_wine['quality'].value_counts()\n",
    "print(\"\\nClass Counts:\")\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# split into training and testing sets\n",
    "X = df_wine.drop('quality', axis=1)\n",
    "y = df_wine['quality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Classes: 6\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(pd.unique(y))\n",
    "print(\"Number of Classes:\", num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.18.0-cp311-cp311-win_amd64.whl.metadata (3.3 kB)\n",
      "Collecting tensorflow-intel==2.18.0 (from tensorflow)\n",
      "  Using cached tensorflow_intel-2.18.0-cp311-cp311-win_amd64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (5.28.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\program files\\windowsapps\\pythonsoftwarefoundation.python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.68.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.0.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow) (0.45.0)\n",
      "Requirement already satisfied: rich in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.1.2)\n",
      "Using cached tensorflow-2.18.0-cp311-cp311-win_amd64.whl (7.5 kB)\n",
      "Using cached tensorflow_intel-2.18.0-cp311-cp311-win_amd64.whl (390.2 MB)\n",
      "Installing collected packages: tensorflow-intel, tensorflow\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\LENOVO\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python311\\\\site-packages\\\\tensorflow\\\\include\\\\external\\\\com_github_grpc_grpc\\\\src\\\\core\\\\ext\\\\filters\\\\client_channel\\\\lb_policy\\\\grpclb\\\\client_load_reporting_filter.h'\n",
      "HINT: This error might have occurred since this system does not have Windows Long Path support enabled. You can find information on how to enable this at https://pip.pypa.io/warnings/enable-long-paths\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.python'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_categorical \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Convert labels to one-hot encoding\u001b[39;00m\n\u001b[0;32m      3\u001b[0m y_one_hot \u001b[38;5;241m=\u001b[39m to_categorical(y \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mmin\u001b[39m(y), num_classes\u001b[38;5;241m=\u001b[39mnum_classes)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\__init__.py:40\u001b[0m\n\u001b[0;32m     37\u001b[0m _os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mENABLE_RUNTIME_UPTIME_TELEMETRY\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasLazyLoader \u001b[38;5;28;01mas\u001b[39;00m _KerasLazyLoader\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.python'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical # type: ignore\n",
    "# Convert labels to one-hot encoding\n",
    "y_one_hot = to_categorical(y - min(y), num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# split into training and testing sets\n",
    "X = df_wine.drop('quality', axis=1)\n",
    "y = df_wine['quality']\n",
    "\n",
    "\n",
    "# First split: Training and testing\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y_one_hot, test_size=0.3, stratify=y, random_state=0\n",
    ")\n",
    "\n",
    "# Second split: Validation and testing from the remaining data\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=0\n",
    ")\n",
    "\n",
    "# Standardize the features\n",
    "sc = StandardScaler()\n",
    "X_train_std = sc.fit_transform(X_train)\n",
    "X_valid_std = sc.transform(X_valid)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1119, 11)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    min_delta=0.001,  # minimum amount of change to count as an improvement\n",
    "    patience=20,      # how many epochs to wait before stopping\n",
    "    restore_best_weights=True,  # restore best weights after stopping\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "apply dropout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there hasn't been at least an improvement of 0.001 in the validation loss over the previous 20 epochs, then stop the training and keep the best model you found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 special layers for preventing overfitting and stanilize training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model whithout optimisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aciocan\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.3620 - loss: 2.9090 - val_accuracy: 0.4292 - val_loss: 1.2157\n",
      "Epoch 2/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4741 - loss: 1.1832 - val_accuracy: 0.4875 - val_loss: 1.1779\n",
      "Epoch 3/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4629 - loss: 1.1872 - val_accuracy: 0.4500 - val_loss: 1.1778\n",
      "Epoch 4/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4600 - loss: 1.1908 - val_accuracy: 0.4625 - val_loss: 1.1652\n",
      "Epoch 5/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5066 - loss: 1.1157 - val_accuracy: 0.4625 - val_loss: 1.1543\n",
      "Epoch 6/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4838 - loss: 1.1610 - val_accuracy: 0.4500 - val_loss: 1.1577\n",
      "Epoch 7/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5176 - loss: 1.1131 - val_accuracy: 0.4833 - val_loss: 1.1633\n",
      "Epoch 8/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4937 - loss: 1.0951 - val_accuracy: 0.4000 - val_loss: 1.2658\n",
      "Epoch 9/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4947 - loss: 1.1234 - val_accuracy: 0.4792 - val_loss: 1.1330\n",
      "Epoch 10/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4980 - loss: 1.1458 - val_accuracy: 0.4042 - val_loss: 1.2073\n",
      "Epoch 11/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5048 - loss: 1.1294 - val_accuracy: 0.4792 - val_loss: 1.1296\n",
      "Epoch 12/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5063 - loss: 1.0790 - val_accuracy: 0.5000 - val_loss: 1.1478\n",
      "Epoch 13/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5246 - loss: 1.1141 - val_accuracy: 0.5083 - val_loss: 1.1025\n",
      "Epoch 14/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5103 - loss: 1.0896 - val_accuracy: 0.4958 - val_loss: 1.1636\n",
      "Epoch 15/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5174 - loss: 1.0625 - val_accuracy: 0.4667 - val_loss: 1.1361\n",
      "Epoch 16/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5376 - loss: 1.1069 - val_accuracy: 0.4875 - val_loss: 1.1139\n",
      "Epoch 17/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4836 - loss: 1.1161 - val_accuracy: 0.4958 - val_loss: 1.1506\n",
      "Epoch 18/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5549 - loss: 1.0504 - val_accuracy: 0.5208 - val_loss: 1.0868\n",
      "Epoch 19/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5450 - loss: 1.0395 - val_accuracy: 0.5167 - val_loss: 1.1768\n",
      "Epoch 20/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5226 - loss: 1.0904 - val_accuracy: 0.5000 - val_loss: 1.1249\n",
      "Epoch 21/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5444 - loss: 1.0806 - val_accuracy: 0.4917 - val_loss: 1.1528\n",
      "Epoch 22/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5501 - loss: 1.0185 - val_accuracy: 0.5208 - val_loss: 1.1071\n",
      "Epoch 23/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5558 - loss: 1.0574 - val_accuracy: 0.5375 - val_loss: 1.0775\n",
      "Epoch 24/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5480 - loss: 1.0304 - val_accuracy: 0.5292 - val_loss: 1.0777\n",
      "Epoch 25/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5430 - loss: 1.0601 - val_accuracy: 0.5125 - val_loss: 1.1148\n",
      "Epoch 26/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5572 - loss: 1.0573 - val_accuracy: 0.5500 - val_loss: 1.0605\n",
      "Epoch 27/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5635 - loss: 1.0288 - val_accuracy: 0.5375 - val_loss: 1.0518\n",
      "Epoch 28/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5920 - loss: 0.9806 - val_accuracy: 0.5417 - val_loss: 1.0501\n",
      "Epoch 29/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5412 - loss: 1.0401 - val_accuracy: 0.5625 - val_loss: 1.0525\n",
      "Epoch 30/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5571 - loss: 1.0826 - val_accuracy: 0.5500 - val_loss: 1.0570\n",
      "Epoch 31/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5503 - loss: 1.0122 - val_accuracy: 0.5292 - val_loss: 1.0914\n",
      "Epoch 32/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5401 - loss: 1.0310 - val_accuracy: 0.5292 - val_loss: 1.0597\n",
      "Epoch 33/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6007 - loss: 0.9889 - val_accuracy: 0.5542 - val_loss: 1.0469\n",
      "Epoch 34/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5337 - loss: 1.0377 - val_accuracy: 0.5375 - val_loss: 1.0478\n",
      "Epoch 35/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5734 - loss: 1.0111 - val_accuracy: 0.5500 - val_loss: 1.0737\n",
      "Epoch 36/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6130 - loss: 0.9527 - val_accuracy: 0.5708 - val_loss: 1.0506\n",
      "Epoch 37/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5635 - loss: 1.0024 - val_accuracy: 0.5917 - val_loss: 1.0562\n",
      "Epoch 38/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5888 - loss: 0.9930 - val_accuracy: 0.6042 - val_loss: 1.0381\n",
      "Epoch 39/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5779 - loss: 0.9780 - val_accuracy: 0.5625 - val_loss: 1.0523\n",
      "Epoch 40/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5570 - loss: 1.0155 - val_accuracy: 0.5583 - val_loss: 1.0393\n",
      "Epoch 41/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5993 - loss: 0.9633 - val_accuracy: 0.5792 - val_loss: 1.0399\n",
      "Epoch 42/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6069 - loss: 0.9768 - val_accuracy: 0.5792 - val_loss: 1.0546\n",
      "Epoch 43/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5916 - loss: 0.9913 - val_accuracy: 0.5458 - val_loss: 1.0581\n",
      "Epoch 44/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5809 - loss: 1.0134 - val_accuracy: 0.6125 - val_loss: 1.0213\n",
      "Epoch 45/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5752 - loss: 0.9795 - val_accuracy: 0.5583 - val_loss: 1.0344\n",
      "Epoch 46/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5895 - loss: 0.9567 - val_accuracy: 0.5375 - val_loss: 1.0701\n",
      "Epoch 47/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6041 - loss: 0.9716 - val_accuracy: 0.5875 - val_loss: 1.0127\n",
      "Epoch 48/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5681 - loss: 0.9567 - val_accuracy: 0.5667 - val_loss: 1.0397\n",
      "Epoch 49/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5903 - loss: 0.9685 - val_accuracy: 0.5708 - val_loss: 1.0267\n",
      "Epoch 50/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6010 - loss: 0.9557 - val_accuracy: 0.5500 - val_loss: 1.0435\n",
      "Test Loss: 0.9908207058906555, Test Accuracy: 0.5791666507720947\n"
     ]
    }
   ],
   "source": [
    "# Multiclass Classification Model\n",
    "multiclass_model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=[X_train.shape[1]]),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "multiclass_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = multiclass_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping], \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = multiclass_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model with optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aciocan\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.1698 - loss: 2.4133 - val_accuracy: 0.4208 - val_loss: 2.1443\n",
      "Epoch 2/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2817 - loss: 2.0216 - val_accuracy: 0.4125 - val_loss: 1.7847\n",
      "Epoch 3/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3344 - loss: 1.8941 - val_accuracy: 0.4125 - val_loss: 1.6065\n",
      "Epoch 4/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3880 - loss: 1.7309 - val_accuracy: 0.4625 - val_loss: 1.4961\n",
      "Epoch 5/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3876 - loss: 1.6849 - val_accuracy: 0.4875 - val_loss: 1.3907\n",
      "Epoch 6/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4282 - loss: 1.5349 - val_accuracy: 0.4708 - val_loss: 1.3356\n",
      "Epoch 7/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4522 - loss: 1.4783 - val_accuracy: 0.4833 - val_loss: 1.2765\n",
      "Epoch 8/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4435 - loss: 1.4800 - val_accuracy: 0.4708 - val_loss: 1.2355\n",
      "Epoch 9/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4966 - loss: 1.3496 - val_accuracy: 0.4958 - val_loss: 1.2080\n",
      "Epoch 10/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4767 - loss: 1.3485 - val_accuracy: 0.5167 - val_loss: 1.1660\n",
      "Epoch 11/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4884 - loss: 1.2529 - val_accuracy: 0.5250 - val_loss: 1.1328\n",
      "Epoch 12/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4750 - loss: 1.2668 - val_accuracy: 0.5083 - val_loss: 1.1156\n",
      "Epoch 13/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5444 - loss: 1.1905 - val_accuracy: 0.5333 - val_loss: 1.0964\n",
      "Epoch 14/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5094 - loss: 1.2053 - val_accuracy: 0.5292 - val_loss: 1.1035\n",
      "Epoch 15/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5136 - loss: 1.2082 - val_accuracy: 0.5500 - val_loss: 1.0600\n",
      "Epoch 16/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5107 - loss: 1.1934 - val_accuracy: 0.5625 - val_loss: 1.0563\n",
      "Epoch 17/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5244 - loss: 1.1424 - val_accuracy: 0.5542 - val_loss: 1.0356\n",
      "Epoch 18/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5499 - loss: 1.1707 - val_accuracy: 0.5542 - val_loss: 1.0205\n",
      "Epoch 19/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5144 - loss: 1.1412 - val_accuracy: 0.5750 - val_loss: 1.0108\n",
      "Epoch 20/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5501 - loss: 1.0998 - val_accuracy: 0.5792 - val_loss: 1.0001\n",
      "Epoch 21/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5224 - loss: 1.1202 - val_accuracy: 0.5750 - val_loss: 1.0151\n",
      "Epoch 22/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5400 - loss: 1.1052 - val_accuracy: 0.5958 - val_loss: 0.9965\n",
      "Epoch 23/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5373 - loss: 1.0943 - val_accuracy: 0.5792 - val_loss: 0.9909\n",
      "Epoch 24/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5338 - loss: 1.0620 - val_accuracy: 0.5917 - val_loss: 0.9957\n",
      "Epoch 25/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5542 - loss: 1.0406 - val_accuracy: 0.5792 - val_loss: 0.9903\n",
      "Epoch 26/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5598 - loss: 1.0677 - val_accuracy: 0.5667 - val_loss: 1.0073\n",
      "Epoch 27/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5266 - loss: 1.0823 - val_accuracy: 0.6000 - val_loss: 0.9861\n",
      "Epoch 28/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5683 - loss: 1.0098 - val_accuracy: 0.5833 - val_loss: 0.9793\n",
      "Epoch 29/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5799 - loss: 1.0187 - val_accuracy: 0.5458 - val_loss: 1.0192\n",
      "Epoch 30/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5538 - loss: 1.0586 - val_accuracy: 0.5500 - val_loss: 1.0094\n",
      "Epoch 31/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5532 - loss: 1.0401 - val_accuracy: 0.5625 - val_loss: 0.9907\n",
      "Epoch 32/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5801 - loss: 1.0041 - val_accuracy: 0.5917 - val_loss: 0.9693\n",
      "Epoch 33/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5423 - loss: 1.0627 - val_accuracy: 0.5625 - val_loss: 1.0051\n",
      "Epoch 34/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5664 - loss: 1.0597 - val_accuracy: 0.5917 - val_loss: 0.9765\n",
      "Epoch 35/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5717 - loss: 1.0210 - val_accuracy: 0.5792 - val_loss: 0.9824\n",
      "Epoch 36/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5754 - loss: 1.0284 - val_accuracy: 0.5625 - val_loss: 0.9995\n",
      "Epoch 37/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5639 - loss: 1.0230 - val_accuracy: 0.5958 - val_loss: 0.9756\n",
      "Epoch 38/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5758 - loss: 0.9894 - val_accuracy: 0.5750 - val_loss: 0.9816\n",
      "Epoch 39/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5404 - loss: 1.0473 - val_accuracy: 0.5708 - val_loss: 0.9939\n",
      "Epoch 40/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5707 - loss: 1.0550 - val_accuracy: 0.5708 - val_loss: 0.9777\n",
      "Epoch 41/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5582 - loss: 1.0357 - val_accuracy: 0.5833 - val_loss: 0.9733\n",
      "Epoch 42/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5399 - loss: 1.0053 - val_accuracy: 0.6167 - val_loss: 0.9703\n",
      "Epoch 43/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5817 - loss: 1.0128 - val_accuracy: 0.5917 - val_loss: 0.9739\n",
      "Epoch 44/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5800 - loss: 1.0251 - val_accuracy: 0.5833 - val_loss: 0.9733\n",
      "Epoch 45/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5934 - loss: 1.0024 - val_accuracy: 0.5917 - val_loss: 0.9670\n",
      "Epoch 46/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5812 - loss: 0.9933 - val_accuracy: 0.5917 - val_loss: 0.9620\n",
      "Epoch 47/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5806 - loss: 0.9992 - val_accuracy: 0.5875 - val_loss: 0.9682\n",
      "Epoch 48/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5384 - loss: 0.9934 - val_accuracy: 0.5792 - val_loss: 0.9721\n",
      "Epoch 49/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5556 - loss: 1.0237 - val_accuracy: 0.5792 - val_loss: 0.9668\n",
      "Epoch 50/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5748 - loss: 1.0341 - val_accuracy: 0.5875 - val_loss: 0.9641\n",
      "Test Loss: 0.9557870626449585, Test Accuracy: 0.6166666746139526\n"
     ]
    }
   ],
   "source": [
    "# Multiclass Classification Model\n",
    "multiclass_model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=[X_train.shape[1]]),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "multiclass_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = multiclass_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping], \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = multiclass_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.2085 - loss: 3.5126 - val_accuracy: 0.3875 - val_loss: 3.2959 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3389 - loss: 2.9693 - val_accuracy: 0.4625 - val_loss: 2.6597 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3745 - loss: 2.6545 - val_accuracy: 0.4542 - val_loss: 2.4538 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4135 - loss: 2.4921 - val_accuracy: 0.4833 - val_loss: 2.2975 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4512 - loss: 2.3533 - val_accuracy: 0.4875 - val_loss: 2.1183 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4115 - loss: 2.2684 - val_accuracy: 0.4917 - val_loss: 2.0292 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5019 - loss: 2.0566 - val_accuracy: 0.5167 - val_loss: 1.9307 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5080 - loss: 1.9774 - val_accuracy: 0.5375 - val_loss: 1.8089 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5091 - loss: 1.9161 - val_accuracy: 0.5375 - val_loss: 1.7435 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5309 - loss: 1.8480 - val_accuracy: 0.5250 - val_loss: 1.6852 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5206 - loss: 1.7616 - val_accuracy: 0.5375 - val_loss: 1.6496 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5259 - loss: 1.7795 - val_accuracy: 0.5500 - val_loss: 1.5931 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5389 - loss: 1.6774 - val_accuracy: 0.5583 - val_loss: 1.5320 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5226 - loss: 1.6471 - val_accuracy: 0.5875 - val_loss: 1.4920 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5284 - loss: 1.5582 - val_accuracy: 0.5708 - val_loss: 1.4626 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5033 - loss: 1.5528 - val_accuracy: 0.5792 - val_loss: 1.4358 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5508 - loss: 1.5055 - val_accuracy: 0.5958 - val_loss: 1.4016 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5591 - loss: 1.4366 - val_accuracy: 0.5500 - val_loss: 1.4029 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5297 - loss: 1.4557 - val_accuracy: 0.5667 - val_loss: 1.3771 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5501 - loss: 1.4271 - val_accuracy: 0.5792 - val_loss: 1.3571 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5414 - loss: 1.3793 - val_accuracy: 0.5917 - val_loss: 1.3330 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5539 - loss: 1.3427 - val_accuracy: 0.5792 - val_loss: 1.3053 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5740 - loss: 1.3075 - val_accuracy: 0.5542 - val_loss: 1.2944 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5683 - loss: 1.3153 - val_accuracy: 0.5625 - val_loss: 1.2810 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5477 - loss: 1.2888 - val_accuracy: 0.5542 - val_loss: 1.2545 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5620 - loss: 1.3021 - val_accuracy: 0.5583 - val_loss: 1.2474 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5554 - loss: 1.2657 - val_accuracy: 0.5917 - val_loss: 1.2208 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5643 - loss: 1.2471 - val_accuracy: 0.5667 - val_loss: 1.2132 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5405 - loss: 1.2683 - val_accuracy: 0.5625 - val_loss: 1.1982 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5626 - loss: 1.1852 - val_accuracy: 0.5583 - val_loss: 1.1859 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5452 - loss: 1.2305 - val_accuracy: 0.5708 - val_loss: 1.1681 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5711 - loss: 1.1636 - val_accuracy: 0.5958 - val_loss: 1.1554 - learning_rate: 0.0010\n",
      "Epoch 33/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5851 - loss: 1.1346 - val_accuracy: 0.5625 - val_loss: 1.1523 - learning_rate: 0.0010\n",
      "Epoch 34/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5422 - loss: 1.1780 - val_accuracy: 0.5583 - val_loss: 1.1405 - learning_rate: 0.0010\n",
      "Epoch 35/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5947 - loss: 1.1690 - val_accuracy: 0.5708 - val_loss: 1.1291 - learning_rate: 0.0010\n",
      "Epoch 36/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5572 - loss: 1.1774 - val_accuracy: 0.5833 - val_loss: 1.1162 - learning_rate: 0.0010\n",
      "Epoch 37/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5811 - loss: 1.1278 - val_accuracy: 0.5667 - val_loss: 1.1133 - learning_rate: 0.0010\n",
      "Epoch 38/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5678 - loss: 1.1302 - val_accuracy: 0.5375 - val_loss: 1.1641 - learning_rate: 0.0010\n",
      "Epoch 39/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5832 - loss: 1.1132 - val_accuracy: 0.5708 - val_loss: 1.0914 - learning_rate: 0.0010\n",
      "Epoch 40/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5668 - loss: 1.1253 - val_accuracy: 0.5667 - val_loss: 1.0838 - learning_rate: 0.0010\n",
      "Epoch 41/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5691 - loss: 1.0983 - val_accuracy: 0.5667 - val_loss: 1.0814 - learning_rate: 0.0010\n",
      "Epoch 42/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5709 - loss: 1.1394 - val_accuracy: 0.5708 - val_loss: 1.0886 - learning_rate: 0.0010\n",
      "Epoch 43/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5743 - loss: 1.0859 - val_accuracy: 0.5667 - val_loss: 1.1095 - learning_rate: 0.0010\n",
      "Epoch 44/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5964 - loss: 1.0264 - val_accuracy: 0.5500 - val_loss: 1.0901 - learning_rate: 0.0010\n",
      "Epoch 45/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5599 - loss: 1.0880 - val_accuracy: 0.5542 - val_loss: 1.1149 - learning_rate: 0.0010\n",
      "Epoch 46/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5850 - loss: 1.0648 - val_accuracy: 0.5917 - val_loss: 1.0447 - learning_rate: 0.0010\n",
      "Epoch 47/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5792 - loss: 1.0519 - val_accuracy: 0.5917 - val_loss: 1.0483 - learning_rate: 0.0010\n",
      "Epoch 48/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5753 - loss: 1.0678 - val_accuracy: 0.5667 - val_loss: 1.0516 - learning_rate: 0.0010\n",
      "Epoch 49/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6000 - loss: 1.0475 - val_accuracy: 0.6042 - val_loss: 1.0481 - learning_rate: 0.0010\n",
      "Epoch 50/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5471 - loss: 1.0568 - val_accuracy: 0.5750 - val_loss: 1.0647 - learning_rate: 0.0010\n",
      "Epoch 51/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5887 - loss: 1.0338 - val_accuracy: 0.5750 - val_loss: 1.0506 - learning_rate: 0.0010\n",
      "Epoch 52/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5351 - loss: 1.1152 - val_accuracy: 0.6000 - val_loss: 1.0316 - learning_rate: 5.0000e-04\n",
      "Epoch 53/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5969 - loss: 0.9882 - val_accuracy: 0.6000 - val_loss: 1.0255 - learning_rate: 5.0000e-04\n",
      "Epoch 54/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5554 - loss: 1.0636 - val_accuracy: 0.5750 - val_loss: 1.0326 - learning_rate: 5.0000e-04\n",
      "Epoch 55/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5666 - loss: 1.0306 - val_accuracy: 0.5708 - val_loss: 1.0349 - learning_rate: 5.0000e-04\n",
      "Epoch 56/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6082 - loss: 1.0157 - val_accuracy: 0.6000 - val_loss: 1.0190 - learning_rate: 5.0000e-04\n",
      "Epoch 57/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5872 - loss: 1.0242 - val_accuracy: 0.5792 - val_loss: 1.0242 - learning_rate: 5.0000e-04\n",
      "Epoch 58/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5754 - loss: 1.0475 - val_accuracy: 0.5833 - val_loss: 1.0213 - learning_rate: 5.0000e-04\n",
      "Epoch 59/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6053 - loss: 1.0209 - val_accuracy: 0.5875 - val_loss: 1.0175 - learning_rate: 5.0000e-04\n",
      "Epoch 60/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5797 - loss: 1.0593 - val_accuracy: 0.5750 - val_loss: 1.0249 - learning_rate: 5.0000e-04\n",
      "Epoch 61/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5785 - loss: 0.9980 - val_accuracy: 0.5875 - val_loss: 1.0136 - learning_rate: 5.0000e-04\n",
      "Epoch 62/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6016 - loss: 0.9887 - val_accuracy: 0.6083 - val_loss: 1.0101 - learning_rate: 5.0000e-04\n",
      "Epoch 63/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5910 - loss: 1.0351 - val_accuracy: 0.5792 - val_loss: 1.0196 - learning_rate: 5.0000e-04\n",
      "Epoch 64/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5699 - loss: 1.0213 - val_accuracy: 0.6000 - val_loss: 1.0148 - learning_rate: 5.0000e-04\n",
      "Epoch 65/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5868 - loss: 0.9948 - val_accuracy: 0.6125 - val_loss: 1.0088 - learning_rate: 5.0000e-04\n",
      "Epoch 66/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5761 - loss: 1.0449 - val_accuracy: 0.6125 - val_loss: 1.0011 - learning_rate: 5.0000e-04\n",
      "Epoch 67/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6025 - loss: 1.0046 - val_accuracy: 0.6208 - val_loss: 0.9933 - learning_rate: 5.0000e-04\n",
      "Epoch 68/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6058 - loss: 0.9777 - val_accuracy: 0.6042 - val_loss: 0.9994 - learning_rate: 5.0000e-04\n",
      "Epoch 69/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5930 - loss: 1.0039 - val_accuracy: 0.6000 - val_loss: 1.0006 - learning_rate: 5.0000e-04\n",
      "Epoch 70/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5849 - loss: 1.0097 - val_accuracy: 0.5792 - val_loss: 1.0039 - learning_rate: 5.0000e-04\n",
      "Epoch 71/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5872 - loss: 0.9847 - val_accuracy: 0.5583 - val_loss: 1.0385 - learning_rate: 5.0000e-04\n",
      "Epoch 72/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5818 - loss: 1.0019 - val_accuracy: 0.6125 - val_loss: 0.9978 - learning_rate: 5.0000e-04\n",
      "Epoch 73/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5909 - loss: 1.0068 - val_accuracy: 0.6083 - val_loss: 0.9952 - learning_rate: 2.5000e-04\n",
      "Epoch 74/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5842 - loss: 0.9786 - val_accuracy: 0.6083 - val_loss: 0.9910 - learning_rate: 2.5000e-04\n",
      "Epoch 75/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6072 - loss: 0.9701 - val_accuracy: 0.6083 - val_loss: 0.9897 - learning_rate: 2.5000e-04\n",
      "Epoch 76/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5879 - loss: 0.9999 - val_accuracy: 0.6042 - val_loss: 0.9867 - learning_rate: 2.5000e-04\n",
      "Epoch 77/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5650 - loss: 0.9810 - val_accuracy: 0.6167 - val_loss: 0.9852 - learning_rate: 2.5000e-04\n",
      "Epoch 78/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5962 - loss: 0.9770 - val_accuracy: 0.6000 - val_loss: 0.9841 - learning_rate: 2.5000e-04\n",
      "Epoch 79/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5783 - loss: 0.9934 - val_accuracy: 0.6000 - val_loss: 0.9826 - learning_rate: 2.5000e-04\n",
      "Epoch 80/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5695 - loss: 1.0171 - val_accuracy: 0.5917 - val_loss: 0.9812 - learning_rate: 2.5000e-04\n",
      "Epoch 81/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6142 - loss: 0.9458 - val_accuracy: 0.5958 - val_loss: 0.9779 - learning_rate: 2.5000e-04\n",
      "Epoch 82/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5785 - loss: 0.9923 - val_accuracy: 0.6083 - val_loss: 0.9803 - learning_rate: 2.5000e-04\n",
      "Epoch 83/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5753 - loss: 1.0019 - val_accuracy: 0.6042 - val_loss: 0.9843 - learning_rate: 2.5000e-04\n",
      "Epoch 84/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5979 - loss: 0.9763 - val_accuracy: 0.5958 - val_loss: 0.9841 - learning_rate: 2.5000e-04\n",
      "Epoch 85/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6070 - loss: 0.9667 - val_accuracy: 0.5917 - val_loss: 0.9856 - learning_rate: 2.5000e-04\n",
      "Epoch 86/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5890 - loss: 0.9555 - val_accuracy: 0.6042 - val_loss: 0.9824 - learning_rate: 2.5000e-04\n",
      "Epoch 87/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5940 - loss: 0.9665 - val_accuracy: 0.6000 - val_loss: 0.9817 - learning_rate: 1.2500e-04\n",
      "Epoch 88/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5903 - loss: 0.9548 - val_accuracy: 0.6083 - val_loss: 0.9788 - learning_rate: 1.2500e-04\n",
      "Epoch 89/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5661 - loss: 1.0034 - val_accuracy: 0.6042 - val_loss: 0.9785 - learning_rate: 1.2500e-04\n",
      "Epoch 90/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5880 - loss: 0.9434 - val_accuracy: 0.5958 - val_loss: 0.9787 - learning_rate: 1.2500e-04\n",
      "Epoch 91/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6072 - loss: 0.9614 - val_accuracy: 0.6042 - val_loss: 0.9783 - learning_rate: 1.2500e-04\n",
      "Test Loss: 0.9479377269744873, Test Accuracy: 0.6166666746139526\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Sequential, layers, regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Model with increased capacity and regularization\n",
    "improved_model = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=[X_train.shape[1]]),  # Larger input layer\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01)),  # Hidden layer with L2 regularization\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.01)),  # Another hidden layer\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(num_classes, activation='softmax')  # Output layer\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "improved_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    min_delta=0.001,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,       # Reduce learning rate by half\n",
    "    patience=5,       # Wait 5 epochs before reducing\n",
    "    min_lr=1e-6       # Minimum learning rate\n",
    ")\n",
    "\n",
    "# Train the improved model\n",
    "history = improved_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the improved model\n",
    "test_loss, test_accuracy = improved_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary Classification Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
